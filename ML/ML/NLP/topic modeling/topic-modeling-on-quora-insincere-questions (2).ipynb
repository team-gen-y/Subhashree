{"cells":[{"metadata":{"_uuid":"10997fdb46ad3033f1d0464f956d11f4a98b2a0d"},"cell_type":"markdown","source":"The objective of this notebook is to discover Quora **insincere** questions' topics, aka target = 1."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\nimport warnings, time, gc\n\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show, output_notebook, reset_output\nfrom bokeh.palettes import d3\nimport bokeh.models as bmo\nfrom bokeh.io import save, output_file\n\nimport re\nimport string\nfrom nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer \n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.manifold import TSNE\n\nfrom wordcloud import WordCloud\n\nnp.random.seed(32)\ncolor = sns.color_palette(\"Set2\")\nwarnings.filterwarnings(\"ignore\")\nstop_words = set(stopwords.words(\"english\"))\npunctuations = string.punctuation\noutput_notebook()\n\n%matplotlib inline\n\ntrain = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f778f945cd8f2fb772f95cef04d5e31c534b57b"},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"175aaa3277627542c4d18144257dcdb823a05ac9"},"cell_type":"markdown","source":"## Target Distrinbution"},{"metadata":{"trusted":true,"_uuid":"7fada13b71497699a10a145e7555fc7c8300d0fd"},"cell_type":"code","source":"target_count = train[\"target\"].value_counts()\n\nplt.figure(figsize = (8, 5))\nax = sns.barplot(target_count.index, target_count.values)\nrects = ax.patches\nlabels = target_count.values\nfor rect, label in zip(rects, labels):\n    ax.text(rect.get_x() + rect.get_width()/2, rect.get_height() + 5,\n           label, ha = \"center\", va = \"bottom\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d460b0544e2cfa0961fefb5ccda9323d4daa2ec5"},"cell_type":"markdown","source":"## Question Length Distribution"},{"metadata":{"trusted":true,"_uuid":"a3070365d9685b31268abb433199d02bed5e60bd"},"cell_type":"code","source":"train[\"quest_len\"] = train[\"question_text\"].apply(lambda x: len(x.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dd319a8fb1ef7901061e73c888598f7580f2466"},"cell_type":"code","source":"sincere = train[train[\"target\"] == 0]\ninsincere = train[train[\"target\"] == 1]\n\nplt.figure(figsize = (15, 8))\nsns.distplot(sincere[\"quest_len\"], hist = True, label = \"sincere\")\nsns.distplot(insincere[\"quest_len\"], hist = True, label = \"insincere\")\nplt.legend(fontsize = 10)\nplt.title(\"Questions Length Distribution by Class\", fontsize = 12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adb7cddcdd42af5441641ab8273a95f52bd7476b"},"cell_type":"markdown","source":"## Data Cleaning"},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true,"_uuid":"25feab011c098ca3190cae7ddda66200278025b4"},"cell_type":"code","source":"#https://drive.google.com/file/d/0B1yuv8YaUVlZZ1RzMFJmc1ZsQmM/view\n# Aphost lookup dict\nAPPO = {\n\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"615155bef3ac73228cbda15e8a378d99dc22da18"},"cell_type":"code","source":"# Credit: https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda\n\nlem = WordNetLemmatizer()\ntokenizer = TweetTokenizer()\n\ndef clean_text(question):\n    \"\"\"\n    This function receives comments and returns clean word-list\n    \"\"\"\n    #Convert to lower case , so that Hi and hi are the same\n    question = question.lower()\n    #remove \\n\n    question = re.sub(\"\\\\n\", \"\", question)\n    #remove disteacting single quotes\n    question = re.sub(\"\\'\", \"\", question)\n    # remove new line characters\n#     question = re.sub('s+', \" \", question)\n    \n    #Split the sentences into words\n    words = tokenizer.tokenize(question)\n    \n    # (')aphostophe  replacement (ie)   you're --> you are  \n    # ( basic dictionary lookup : master dictionary present in a hidden block of code)\n    words = [APPO[word] if word in APPO else word for word in words]\n    words = [lem.lemmatize(word, \"v\") for word in words]\n    words = [w for w in words if w not in stop_words and w not in punctuations]\n\n    clean_sent = \" \".join(words)\n    # remove any non alphanum, digit character\n#     clean_sent = re.sub(\"\\W+\", \" \", clean_sent)\n#     clean_sent = re.sub(\"  \", \" \", clean_sent)\n    \n    return clean_sent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29b9f26c84e79e04c7299b64bc3ab3b9ecf7b2a6"},"cell_type":"code","source":"sincere[\"clean_question_text\"] = sincere[\"question_text\"].apply(lambda question: clean_text(question))\ninsincere[\"clean_question_text\"] = insincere[\"question_text\"].apply(lambda question: clean_text(question))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cb3ee1d0458e301861b3f8a60599ba0a1e4dfef"},"cell_type":"code","source":"insincere.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e9397fadd93608d3088020d5b954496b3481fbb"},"cell_type":"markdown","source":"## Insincere Questions Topic Modeling"},{"metadata":{"trusted":true,"_uuid":"187a181d236e2dc3321d1349df57f3be1c8485ab"},"cell_type":"code","source":"cv = CountVectorizer(min_df = 10,\n                     max_features = 100000,\n                     analyzer = \"word\",\n                     ngram_range = (1, 2),\n                     stop_words = \"english\",\n                     token_pattern = '[a-zA-Z]')\n\ncount_vectors = cv.fit_transform(insincere[\"clean_question_text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34fab3191b36b5ef84de692a5ba081a511da977a"},"cell_type":"code","source":"# params = {\"n_components\": [5, 10, 20, 30, 40, 50]}\n\n# lda_model = LatentDirichletAllocation(n_components = n_topics, \n#                                       # we choose a small n_components for time convenient\n#                                       # will find a appropriate n_components later \n#                                       learning_method = \"online\",\n#                                       batch_size = 128,\n#                                       evaluate_every = -1,\n#                                       max_iter = 20,\n#                                       random_state = 32,\n#                                       n_jobs = -1)\n\n# model = GridSearchCV(lda_model, param_grid = params)\n# model.fit(count_vectors)\n\n# best_lda_model = model.best_estimator_\n# best_lda_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21bbad75b1b646a4c93ad45cedb4aaf19e058ace"},"cell_type":"markdown","source":"After applying Grid Search, we found the optimial **n_components** is between 5 to 10. In this case, we pick the 'mean' which is 8."},{"metadata":{"trusted":true,"_uuid":"248358495763c22c0eeb5dd6e8a6804e9be316b3"},"cell_type":"code","source":"n_topics = 8\nlda_model = LatentDirichletAllocation(n_components = n_topics, \n                                      learning_method = \"online\",\n                                      batch_size = 128,\n                                      evaluate_every = -1,\n                                      max_iter = 20,\n                                      random_state = 32,\n                                      n_jobs = -1)\n\nquestion_topics = lda_model.fit_transform(count_vectors)\ntemp = question_topics","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d48bc7deb8457de9fc663937c99f2cd5e4a3c1d0"},"cell_type":"markdown","source":"To get a better LDA model, we need to maximize log likelihood and minimize perplexity."},{"metadata":{"trusted":true,"_uuid":"a6ec5c1413217bf9f9b6e0678e9d7e54a74f0eb7"},"cell_type":"code","source":"print(\"Log Likelihood: {} \\nPerplexity: {}\".format(lda_model.score(count_vectors), \n                                                   lda_model.perplexity(count_vectors)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"854f322fcca808898329abca04b1262b871ddad3","_kg_hide-output":true},"cell_type":"code","source":"tsne_model = TSNE(n_components = 2, verbose = 1, random_state = 32, n_iter = 500)\ntsne_lda = tsne_model.fit_transform(question_topics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0a7866e1c788b7acd4bd4230944171fe3f653d5"},"cell_type":"code","source":"question_topics = np.matrix(question_topics)\ndoc_topics = question_topics/question_topics.sum(axis = 1)\n\nlda_keys = []\nfor i, tweet in enumerate(insincere[\"question_text\"]):\n    lda_keys += [doc_topics[i].argmax()]\n    \ntsne_lda_df = pd.DataFrame(tsne_lda, columns = [\"x\", \"y\"])\ntsne_lda_df[\"qid\"] = insincere[\"qid\"].values\ntsne_lda_df[\"question\"] = insincere[\"question_text\"].values\ntsne_lda_df[\"topics\"] = lda_keys\ntsne_lda_df[\"topics\"] = tsne_lda_df[\"topics\"].map(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7382c0155e409ef8b6266bf13045d6eea45278ff"},"cell_type":"code","source":"import random\n\ndef generate_color():\n    color = \"#{:02x}{:02x}{:02x}\".format(*map(lambda x: random.randint(0, 255), range(3)))\n    return color","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67f72415980541ca9268090fe1128d3f1031295b"},"cell_type":"code","source":"colormap = np.array([generate_color() for t in range(n_topics)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efbc9e28ae138c779b4246b0df08c81d71be285b"},"cell_type":"code","source":"plot_lda = bp.figure(plot_width = 700, plot_height = 600, \n                    title = \"LDA topics of Quora Questions\",\n                    tools = \"pan, wheel_zoom, box_zoom, reset, hover, previewsave\",\n                    x_axis_type = None, y_axis_type = None, min_border = 1)\n\nsource = ColumnDataSource(data = dict(x = tsne_lda_df[\"x\"], y = tsne_lda_df[\"y\"],\n                         color = colormap[lda_keys],\n                         qid = tsne_lda_df[\"qid\"],\n                         question = tsne_lda_df[\"question\"],\n                         topics = tsne_lda_df[\"topics\"]))\n\nplot_lda.scatter(x = \"x\", y = \"y\", color = \"color\", source = source)\nhover = plot_lda.select(dict(type = HoverTool))\nhover.tooltips = {\"qid\": \"@qid\",\"question\": \"@question\", \"topics\": \"@topics\"}\nshow(plot_lda)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c1d5b467e23aef5e4cb7ef6870e655823d547ac"},"cell_type":"markdown","source":"Although we can see some patterns in the visualization from above, the graph is difficult to interpret.  The very reason for that is our model is unable to confidently assign a topic to every questions. This means that there are questions being assigned a low probability to a probable topic. To filter out such questions, we simply add a threshold factor."},{"metadata":{"_uuid":"e69efe42d4cbc6d63cb7a5d5c6807bafe1c3eb34"},"cell_type":"markdown","source":"#### Topic Probability => 0.5"},{"metadata":{"trusted":true,"_uuid":"45e941e5b4c6fb1a2003dfaa79a342eae18becf2"},"cell_type":"code","source":"threshold = 0.5\nidx = np.amax(temp, axis = 1) >= threshold\nquestion_topics = temp[idx]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"d2e0493658924f4fb0f151dafa036f247a2ba562"},"cell_type":"code","source":"tsne_model = TSNE(n_components = 2, verbose = 1, random_state = 32, n_iter = 500)\ntsne_lda2 = tsne_model.fit_transform(question_topics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9534991bbe2dc05ef8ebcace01f0da8623be0e2f"},"cell_type":"code","source":"new_insincere = insincere[[\"qid\", \"question_text\"]].copy()\nnew_insincere = new_insincere[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed9566bb6920b730f216cd14157188f96ada47eb"},"cell_type":"code","source":"question_topics = np.matrix(question_topics)\ndoc_topics = question_topics/question_topics.sum(axis = 1)\n\nlda_keys = []\nfor i, tweet in enumerate(new_insincere[\"question_text\"]):\n    lda_keys += [doc_topics[i].argmax()]\n    \ntsne_lda_df2 = pd.DataFrame(tsne_lda2, columns = [\"x\", \"y\"])\ntsne_lda_df2[\"qid\"] = new_insincere[\"qid\"].values\ntsne_lda_df2[\"question\"] = new_insincere[\"question_text\"].values\ntsne_lda_df2[\"topics\"] = lda_keys\ntsne_lda_df2[\"topics\"] = tsne_lda_df2[\"topics\"].map(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"865da9050d2247307241d6978c6675cadfddf345"},"cell_type":"code","source":"plot_lda = bp.figure(plot_width = 700, plot_height = 600, \n                    title = \"LDA topics of Quora Questions\",\n                    tools = \"pan, wheel_zoom, box_zoom, reset, hover, previewsave\",\n                    x_axis_type = None, y_axis_type = None, min_border = 1)\n\nsource = ColumnDataSource(data = dict(x = tsne_lda_df2[\"x\"], y = tsne_lda_df2[\"y\"],\n                         color = colormap[lda_keys],\n                         qid = tsne_lda_df2[\"qid\"],\n                         question = tsne_lda_df2[\"question\"],\n                         topics = tsne_lda_df2[\"topics\"]))\n\nplot_lda.scatter(x = \"x\", y = \"y\", color = \"color\", source = source)\nhover = plot_lda.select(dict(type = HoverTool))\nhover.tooltips = {\"qid\": \"@qid\", \"question\": \"@question\", \"topics\": \"@topics\"}\nshow(plot_lda)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c17cdd304d55eaccfe57662c558d46fbbad98be0"},"cell_type":"markdown","source":"We get a much better visualization after using probability threshold."},{"metadata":{"_uuid":"1097543051d1a1bd85c0ca1f9f06ec3a072a11ac"},"cell_type":"markdown","source":"#### Topic Probability < 0.5"},{"metadata":{"trusted":true,"_uuid":"2bcc681eabc752fab9a27b94e50d406a14725b55"},"cell_type":"code","source":"idx = np.amax(temp, axis = 1) < threshold\nquestion_topics = temp[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"7303f6faeea2087bd603a0503f3ffd7e7959546d","_kg_hide-output":true},"cell_type":"code","source":"tsne_model = TSNE(n_components = 2, verbose = 1, random_state = 32, n_iter = 500)\ntsne_lda3 = tsne_model.fit_transform(question_topics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"904007f548f54c058c7797ff94b7615d39264694"},"cell_type":"code","source":"new_insincere2 = insincere[[\"qid\", \"question_text\"]].copy()\nnew_insincere2 = new_insincere2[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d85cfe90a0497465cace962c425ecf46f0afb3c3"},"cell_type":"code","source":"question_topics = np.matrix(question_topics)\ndoc_topics = question_topics/question_topics.sum(axis = 1)\n\nlda_keys = []\nfor i, tweet in enumerate(new_insincere2[\"question_text\"]):\n    lda_keys += [doc_topics[i].argmax()]\n    \ntsne_lda_df3 = pd.DataFrame(tsne_lda3, columns = [\"x\", \"y\"])\ntsne_lda_df3[\"qid\"] = new_insincere2[\"qid\"].values\ntsne_lda_df3[\"question\"] = new_insincere2[\"question_text\"].values\ntsne_lda_df3[\"topics\"] = lda_keys\ntsne_lda_df3[\"topics\"] = tsne_lda_df2[\"topics\"].map(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b10fbf8ddd7ce9711baac39235769714957300d"},"cell_type":"code","source":"plot_lda = bp.figure(plot_width = 700, plot_height = 600, \n                    title = \"LDA topics of Quora Questions\",\n                    tools = \"pan, wheel_zoom, box_zoom, reset, hover, previewsave\",\n                    x_axis_type = None, y_axis_type = None, min_border = 1)\n\nsource = ColumnDataSource(data = dict(x = tsne_lda_df3[\"x\"], y = tsne_lda_df3[\"y\"],\n                         color = colormap[lda_keys],\n                         qid = tsne_lda_df3[\"qid\"],\n                         question = tsne_lda_df3[\"question\"],\n                         topics = tsne_lda_df3[\"topics\"]))\n\nplot_lda.scatter(x = \"x\", y = \"y\", color = \"color\", source = source)\nhover = plot_lda.select(dict(type = HoverTool))\nhover.tooltips = {\"qid\": \"@qid\", \"question\": \"@question\", \"topics\": \"@topics\"}\nshow(plot_lda)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4b18903259e474b4e3238bdd2e058997c06f7d1"},"cell_type":"markdown","source":"## Insincere Topic Wordcloud"},{"metadata":{"trusted":true,"_uuid":"13d759154b5790327cbd302972627923b679f1b2","scrolled":false},"cell_type":"code","source":"def create_wordcloud(i, data):\n#     plt.subplot(int(\"52{}\".format(ax+1)))\n    wc =  WordCloud(max_words = 1000, stopwords = stop_words)\n    wc.generate(\" \".join(data))\n    ax[int(i/2)][i%2].axis(\"off\")\n    ax[int(i/2)][i%2].set_title(\"Words Frequented in Topic {}\".format(i), fontsize = 15)\n    ax[int(i/2)][i%2].imshow(wc)\n    \nfig, ax = plt.subplots(4, 2, figsize = (25, 25))\nfor i in range(n_topics):\n    text = tsne_lda_df[tsne_lda_df[\"topics\"] == int(i)][\"question\"]\n    create_wordcloud(int(i), text)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd3ff1ce1154b3fdcf6ed980b8c86ce3a837af86"},"cell_type":"markdown","source":"## Topic Network"},{"metadata":{"_uuid":"5fec9f8f8e87cc004aacdd0a58f915facefce999"},"cell_type":"markdown","source":"Related paper: [Topic Modeling and Network Visualization to\nExplore Patient Experiences](http://faculty.washington.edu/atchen/pubs/Chen_Sheble_Eichler_VAHC2013.pdf)"},{"metadata":{"trusted":true,"_uuid":"8b154972686bcf1d3ed57dd7e5288d50097d64cf"},"cell_type":"code","source":"import networkx as nx\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom scipy.spatial.distance import pdist, squareform\n\ncor = squareform(pdist(tsne_lda2, metric = \"euclidean\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7468037b6e37cdabf42bf35a0b2ee1aad5c7a839"},"cell_type":"code","source":"cor = squareform(pdist(tsne_lda2[:100], metric = \"euclidean\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3bd4a29813753105fbf2f68f7859607f0621213"},"cell_type":"code","source":"labels = {}\n\nfor l, i in enumerate(tsne_lda_df2[\"qid\"]):\n    labels[l] = i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"664c24fa1c475ebb46428badcebab6cf52b50811","scrolled":false},"cell_type":"code","source":"G = nx.Graph()\n\nfor i in range(cor.shape[0]):\n    for j in range(cor.shape[1]):\n        if i == j:\n            G.add_edge(i, j, weight = 0)\n        else:\n            G.add_edge(i, j, weight = 1.0/cor[i, j])\n            \nG = nx.relabel_nodes(G, labels)\n            \nedges = [(i, j) for i, j, w in G.edges(data = True) if w[\"weight\"] > 0.8]\nedge_weight = dict([((u, v, ), int(d[\"weight\"])) for u, v, d in G.edges(data = True)])\n\npos = nx.spring_layout(G)\n\nplt.figure(figsize = (10, 8))\nnx.draw_networkx_nodes(G, pos, node_size = 100, alpha = 0.5)\nnx.draw_networkx_edges(G, pos, edgelist = edges, width = 1)\nnx.draw_networkx_labels(G, pos, font_size = 8, font_family = \"sans-serif\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b69d09a0a605e18b89d934694d666c929f08cab7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe57058a72d00102661a7df0dbc0ea8ba0281356"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a903a3e246a9718ec5ac289a2e16928e8578f9f"},"cell_type":"markdown","source":"# To Be Continued ..."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}